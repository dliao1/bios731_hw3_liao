---
title: 'Homework 3'
header-includes:
  - \usepackage{multirow}
output:
  pdf_document: default
urlcolor: blue
---

```{r, include=FALSE}

library(tidyverse)
library(here)
library(tidyr)
library(knitr)
library(dplyr)
library(ggplot2)
library(dplyr)
library(cowplot)
library(kableExtra)
library(patchwork)


knitr::opts_chunk$set(tidy = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(echo = FALSE)

source(here("source", "process_results.R"))
```

## Context

This assignment reinforces ideas in Module 3: Cluster computing.


## Due date and submission

Please submit (via Canvas) a PDF containing a link to the web address of the GitHub repo containing your work for this assignment; git commits after the due date will cause the assignment to be considered late. 



## Points

```{r, echo = FALSE}
tibble(
  Problem = c("Problem 0", "Problem 1"),
  Points = c(20, 80)
) %>%
  knitr::kable()
```


## Problem 0 

This “problem” focuses on structure of your submission, especially the use git and GitHub for reproducibility, R Projects to organize your work, R Markdown to write reproducible reports, relative paths to load data from local files, and reasonable naming structures for your files.

To that end:

* Create a public GitHub repo + local R Project
* Submit your whole project folder to GitHub 
* Submit a PDF knitted from Rmd to Canvas. Your solutions to the problems here should be implemented in your .Rmd file, and your git commit history should reflect the process you used to solve these Problems.


## Problem 1

Continuation of Homework 1. Here, we will re-run part of the simulation study from Homework 1 with some minor changes, on the cluster. Cluster computing space is limited so we will not run too many jobs or simulations.  

### Problem 1 setup

The simulation study is specified below:

Below is a multiple linear regression model, where we are interested in primarily treatment effect.


$$Y_i = \beta_0 + \beta_{treatment}X_{i1} + \mathbf{Z_i}^T\boldsymbol{\gamma} + \epsilon_i$$


Notation is defined below: 

* $Y_i$: continuous outcome
* $X_{i1}$: treatment group indicator; $X_{i1}=1$ for treated 
* $\mathbf{Z_i}$: vector of potential confounders
* $\beta_{treatment}$: average treatment effect, adjusting for $\mathbf{Z_i}$
* $\boldsymbol{\gamma}$: vector of regression coefficient values for confounders 
* $\epsilon_i$: errors, we will vary how these are defined


In our simulation, we want to 

* Estimate $\beta_{treatment}$
  * Evaluate $\beta_{treatment}$ through bias, coverage, type 1 error, and power
  * We will use 2 methods to compute $se(\hat{\beta}_{treatment})$ and coverage:
    1. Wald confidence intervals (the standard approach)
    2. Nonparametric bootstrap percentile intervals
  * Evaluate computation times for each method to compute a confidence interval

* Evaluate these properties at:
  - Sample size $n =\{20\}$
  - True values $\beta_{treatment} \in \{0, 0.5\}$
  - True $\epsilon_i$ normally distributed with $\epsilon_i \sim N(0, 2)$
  - True $\epsilon_i$ coming from a highly right skewed distribution
    - Generate data from a Gamma distribution with `shape = 1` and `rate = 2`.

* Assume that there are no confounders ($\boldsymbol{\gamma} = 0$)
* Use a full factorial design
* Use same `nsim` as previous assignment.


### Problem 1 tasks

We will execute this full simulation study. For full credit, make sure to implement the following:

**Workflow:**
* Use structured scripts and subfolders following guidance from the cluster computing project organization lecture
* Instead of parallelizing your simulation scenarios (as in HW1), each simulation scenario should be assigned a different JOBID on the cluster.


**Presenting results:**

Create plots with *Monte Carlo standard error bars* to summarize the following:

- Bias of $\hat{\beta}$
- Coverage of $\hat{\beta}$
- Power
- Type 1 error

Write 1-2 paragraphs summarizing these results.

```{r}
num_scenarios <- 1 * 2 * 2

mc_err <- 0.01
cover <- 0.95
alpha <- 1 - 0.95
n_sim <- (cover * (1 - cover))/mc_err^2
```

```{r}

df <- process_results(num_scenarios, n_sim)
```

```{r}
bias_table <- df %>%
  select(n, beta_true, error_type, bias) %>%  
  arrange(n, beta_true) %>%
  rename("N" = n, "True Beta" = beta_true) %>%
  pivot_wider(
    names_from = error_type,
    values_from = bias,   
  )

kable(bias_table, digits = 4, caption = "Bias Summary Table")
```

```{r}
ggplot(df, aes(x = error_type, y = bias, color = as.factor(beta_true))) +
  geom_point(position = position_dodge(width = 0.3), size = 3) + 
  geom_errorbar(aes(ymin = bias - bias_mcse, ymax = bias + bias_mcse), 
                position = position_dodge(width = 0.3), width = 0.2) + 
  labs(title = "Bias Summary Plot",
       x = "Error Type",
       y = "Bias",
       color = "True Beta") +
  theme_minimal()
```


```{r}
coverage_table <- df %>%
  select(n, beta_true, error_type, wald_coverage, boot_p_coverage) %>%
  arrange(n, beta_true, error_type) %>%
  pivot_wider(
    names_from = error_type,  
    values_from = c(wald_coverage, boot_p_coverage),
    names_glue = "{error_type} {.value}", 
  ) %>%
  select(n, beta_true, starts_with("Gamma"), starts_with("Normal")) %>%
  rename(
    "N" = n,
    "True Beta" = beta_true,
    "Gamma Wald CI" = "Gamma wald_coverage",
    "Gamma Bootstrap Percentile CI" = "Gamma boot_p_coverage",
    "Normal Wald CI" = "Normal wald_coverage",
    "Normal Bootstrap Percentile CI" = "Normal boot_p_coverage",
  )


kable(coverage_table, digits = 3, caption = "Coverage Summary Table") %>%
  add_header_above(c(" " = 2, "Gamma" = 2, "Normal" = 2)) %>%
  column_spec(1, width = "1cm") %>%
  column_spec(2, width = "1cm") %>%
  column_spec(3:6, width = "2cm") 

```


```{r}
df_coverage <- df %>%
  select(error_type, beta_true, wald_coverage, boot_p_coverage, wald_coverage_mcse, boot_p_coverage_mcse) %>%
  pivot_longer(cols = c(wald_coverage, boot_p_coverage),
               names_to = "method", 
               values_to = "coverage") %>%
  mutate(method = recode(method, 
                         wald_coverage = "Wald", 
                         boot_p_coverage = "Bootstrap Percentile"),
         mcse = ifelse(method == "Wald", wald_coverage_mcse, boot_p_coverage_mcse))

ggplot(df_coverage, aes(x = error_type, y = coverage, color = as.factor(beta_true), shape = method)) +
  geom_point(position = position_dodge(width = 0.4), size = 4) +  
  geom_errorbar(aes(ymin = coverage - mcse, ymax = coverage + mcse), 
                position = position_dodge(width = 0.4), width = 0.2) +  
  labs(title = "Wald vs. Bootstrap CI Coverage (by error type)",
       x = "Error Type", y = "Coverage", 
       color = "True Beta", shape = "Method") +
  theme_minimal(base_size = 14) +
  theme(legend.position = "bottom",
        plot.title = element_text(face = "bold", size = 16),
        axis.text = element_text(size = 12),
        axis.title = element_text(size = 14))
```



```{r}
df_type1 <- df %>%
  select(error_type, beta_true, wald_type1, boot_p_type1, wald_type1_mcse, boot_p_type1_mcse) %>%
  pivot_longer(cols = c(wald_type1, boot_p_type1),
               names_to = "method", 
               values_to = "type1") %>%
  mutate(method = recode(method, 
                         wald_type1 = "Wald", 
                         boot_p_type1 = "Bootstrap Percentile"),
         mcse = ifelse(method == "Wald", wald_type1_mcse, boot_p_type1_mcse))

ggplot(df_type1, aes(x = error_type, y = type1, color = as.factor(beta_true), shape = method)) +
  geom_point(position = position_dodge(width = 0.4), size = 4) +  
  geom_errorbar(aes(ymin = type1 - mcse, ymax = type1 + mcse), 
                position = position_dodge(width = 0.4), width = 0.2) +  
  labs(title = "Wald vs Bootstrap Type I Error Estimates",
       x = "Error Type", y = "Type I Error", 
       color = "True Beta", shape = "Method") +
  theme_minimal(base_size = 14) +
  theme(legend.position = "bottom",
        plot.title = element_text(face = "bold", size = 16),
        axis.text = element_text(size = 12),
        axis.title = element_text(size = 14))

```



```{r}
df_power <- df %>%
  select(error_type, beta_true, wald_power, boot_p_power, wald_power_mcse, boot_p_power_mcse) %>%
  pivot_longer(cols = c(wald_power, boot_p_power),
               names_to = "method", 
               values_to = "power") %>%
  mutate(method = recode(method, 
                         wald_power = "Wald", 
                         boot_p_power = "Bootstrap Percentile"),
         mcse = ifelse(method == "Wald", wald_power_mcse, boot_p_power_mcse))

ggplot(df_power, aes(x = error_type, y = power, color = as.factor(beta_true), shape = method)) +
  geom_point(position = position_dodge(width = 0.4), size = 4) +  
  geom_errorbar(aes(ymin = power - mcse, ymax = power + mcse), 
                position = position_dodge(width = 0.4), width = 0.2) +  
  labs(title = "Wald vs Bootstrap Power Estimates",
       x = "Error Type", y = "Power", 
       color = "True Beta", shape = "Method") +
  theme_minimal(base_size = 14) +
  theme(legend.position = "bottom",
        plot.title = element_text(face = "bold", size = 16),
        axis.text = element_text(size = 12),
        axis.title = element_text(size = 14))

```

### Summary

To summarize, it looks like the bias for normal errors tended to be more negative,
at around 0.012, while bias for gamma errors tended to be positive, at around
-0.012. The Monte Carlo standard errors for normal errors looked to be larger than 
the MCSE for gamma errors, which makes sense, and these conclusions make sense
when considering that the gamma distribution takes on only positive values, and that
the variance of a gamma distribution with shape = 1 and rate = 2 is a lot
smaller than the variance of 2 that we used for the normal distribution. 

For coverages, Wald confidence intervals tended to have higher coverage for both
true betas of 0 and 1 across both gamma and normal errors.

As for type I error and power, it looks like bootstrap percentile intervals had higher
type I error rates than the wald intervals. For some reason, both bootstrap percentile
intervals and wald intervals had really low power for both true betas of 0 and 
0.5 for normal errors, but much higher power for gamma errors. My hypothesis
was that Wald intervals would have higher power than the bootstrap intervals
for the normal errors especially, but maybe due to the original sample size being
n = 20 and the variance being higher than that of the gamma distribution, that was 
why the power was so low for the normal errors.
